---
title: 'Blog Post #4'
author: Ella Trembanis
date: '2024-09-29'
output:
  html_document:
    df_print: paged
categories: []
tags: []
slug: "blog-post-4"
---

# The Incumbency Advantage(s)

Roughly two-thirds of all presidential elections in the postwar period were won by incumbents. 2024 will not be one such election year, but even without a sitting president on the ticket, the question of incumbency looms larger than ever.

To call incumbency an advantage is a little misleading; especially for a year like this one, it is useful to think of incumbency as a network of often conjoined – but theoretically separable – conditions. There’s name recognition, the ability to direct federal grants, access to the bully pulpit, avoidance of the primary gauntlet, and having already survived one general election, to name a few.

So…who is the incumbent-iest of the 2024 candidates?

Obviously Trump, right? He is a former president, has extensive name-recognition, and was not seriously derailed by his few co-partisan challengers in the primaries.

Or maybe it should be Harris: she’s the sitting Vice President, after all, and she inherited much of Biden’s campaign machinery when he withdrew from consideration. Although…she fell quite a ways short of the nomination in 2020, dropping out early on during primary season, so she hasn’t been thoroughly battle-tested via a national election.

Ultimately, the facets of “incumbency” are not neatly united in either candidate this year, leaving both to awkwardly stake claims to being the real changemaker on the ballot. Rather than using any single measure of incumbency status, the safest option is probably to break it down into its component parts – to the extent that we can identify them – and systematically distribute each one to the best-suited candidate.

Take, for instance, the allocation of federal grant money – also known as pork – to certain constituencies. The Biden administration has had access to this benefit of office for the last several years. Since, until recently, Biden was running for re-election himself, it seems likely that his spending strategy would fall roughly in line with the pattern depicted in Figure I (below): spending relatively more than he would had Harris been the 2024 candidate from the start.

```{r load libraries, echo=FALSE, include=FALSE, warning=FALSE}
library(ggplot2)
library(mice)
library(car)
library(caret)
library(CVXR)
library(glmnet)
library(kableExtra)
library(maps)
library(RColorBrewer)
library(sf)
library(tidyverse)
library(viridis)
```

```{r custom theme, echo=FALSE, include=FALSE, warning=FALSE}
my_theme <- function() {
  theme(
  # set panel features
  panel.border = element_rect(color = "#000033", fill = NA, linetype = 1),
  panel.background = element_rect(fill = "#FFFFFF"),
  panel.grid.major.x = element_line(color = "#9999CC", linetype = 2, size = 0.2),
  panel.grid.minor.x = element_blank(),
  panel.grid.major.y = element_line(color = "#9999CC", linetype = 2, size = 0.2),
  panel.grid.minor.y = element_blank(),
  # set text and axis features
  text = element_text(color = "#000033", family = "Courier"),
  title = element_text(color = "#000033", face = "bold", family = "Courier"),
  axis.ticks = element_line(color = "#000033"),
  # set legend features
  legend.position = "bottom",
  legend.background = element_rect(color = "#000033", fill = NA, linetype = 1)
  )
}
```

```{r read data, echo=FALSE, include=FALSE, warning=FALSE}
# processed FiveThirtyEight polling average datasets
nat_poll <- read_csv("/Users/ellatrembanis/Desktop/Gov 1347/election-blog/election-blog/Blog #3/data/national_polls_1968-2024.csv")
state_poll <- read_csv("/Users/ellatrembanis/Desktop/Gov 1347/election-blog/election-blog/Week4/data/state_polls_1968-2024.csv")

# national popular vote
nat_vote <- read_csv("/Users/ellatrembanis/Desktop/Gov 1347/election-blog/election-blog/Week4/data/popvote_1948-2020(3).csv") |>
  filter(year >= 1968)
nat_vote$party[nat_vote$party == "democrat"] <- "DEM"
nat_vote$party[nat_vote$party == "republican"] <- "REP"

# state popular vote
state_vote <- read_csv("/Users/ellatrembanis/Desktop/Gov 1347/election-blog/election-blog/Blog #3/data/clean_wide_state_2pv_1948_2020.csv") |>
  filter(year >= 1968) |>
  select(year, state, D_pv2p, R_pv2p) |>
  pivot_longer(
    cols = D_pv2p:R_pv2p,
    cols_vary = "fastest",
    names_to = ("party")
  )
state_vote$party[state_vote$party == "D_pv2p"] <- "DEM"
state_vote$party[state_vote$party == "R_pv2p"] <- "REP"
state_vote$pv2p <- state_vote$value

state_vote <- state_vote |>
  select(year, state, party, pv2p)

# Read electoral college data.
ec <- read_csv("/Users/ellatrembanis/Desktop/Gov 1347/election-blog/election-blog/Week4/data/corrected_ec_1948_2024.csv")

# Read economic data.
econ <- read_csv("/Users/ellatrembanis/Desktop/Gov 1347/election-blog/election-blog/Blog #3/data/fred_econ.csv") |> 
# filter to most recent available quarter of 2024
  filter(quarter == 2)

# Read consumer sentiment index
ics <- read_csv("/Users/ellatrembanis/Desktop/Gov 1347/election-blog/election-blog/Week4/data/tbqics.csv") |>
  filter(quarter == "Q2") |>
  mutate(
    quarter = if_else(quarter == "Q2", 2, NA)
    )

# Merge econ + ics
econ <- econ |>
  left_join(ics, by = join_by(year, quarter))

# Shape and merge polling and election data using November polls. 
poll_nov <- nat_vote |> 
  left_join(nat_poll |> 
              group_by(year, party) |> 
              top_n(1, poll_date) |> 
              select(-candidate), 
            by = c("year", "party")) |> 
  rename(nov_poll = poll_support) |> 
  filter(year <= 2020) |> 
  drop_na()

# Create dataset of polling average by week until the election. 
week_poll <- nat_poll |> 
  group_by(year, party, weeks_left) |>
  summarize(mean_poll_week = mean(poll_support)) |> 
  filter(weeks_left <= 30) |> 
  pivot_wider(names_from = weeks_left, values_from = mean_poll_week) |> 
  left_join(nat_vote, by = c("year", "party"))

# Read federal grants dataset from Kriner & Reeves (2008). 
pork_state <- read_csv("/Users/ellatrembanis/Desktop/Gov 1347/election-blog/election-blog/Week4/data/fedgrants_bystate_1988-2008.csv")
pork_county <- read_csv("/Users/ellatrembanis/Desktop/Gov 1347/election-blog/election-blog/Week4/data/fedgrants_bycounty_1988-2008.csv")

# Join data for time for change model.
tfc_train <- nat_vote |> 
  left_join(econ, by = "year") |> 
  filter(incumbent_party) |>
  mutate(incumbent = as.numeric(incumbent))

# Recode Harris as an incumbent party heir (incumbent = 0, incumbent_party = TRUE).
tfc_train[15, "incumbent"] = 0
```

```{r modelling, echo=FALSE, include=FALSE, warning=FALSE}
# use weeks left as columns
week_poll <- nat_poll |> 
  group_by(year, party, weeks_left) |>
  summarize(mean_poll_week = mean(poll_support)) |> 
  filter(weeks_left <= 30) |> 
  pivot_wider(names_from = weeks_left, values_from = mean_poll_week) |>
# merge national vote
  left_join(nat_vote, by = c("year", "party"))

# split to train and test
week_poll_train <- week_poll |> 
  filter(year <= 2020)
week_poll_test <- week_poll |> 
  filter(year == 2024)

# rename
colnames(week_poll)[3:33] <- paste0("poll_weeks_left_", 0:30)
colnames(week_poll_train)[3:33] <- paste0("poll_weeks_left_", 0:30)
colnames(week_poll_test)[3:33] <- paste0("poll_weeks_left_", 0:30)

# define x (polls by weeks left) and y (popular vote)
x.train <- week_poll_train |>
  ungroup() |> 
  select(all_of(starts_with("poll_weeks_left_"))) |> 
  as.matrix()
y.train <- week_poll_train$pv2p

# ols
ols.weekpoll <- lm(paste0("pv2p ~ ", paste0( "poll_weeks_left_", 0:30, collapse = " + ")), 
                    data = week_poll_train)
# set ridge using alpha = 0
ridge.weekpoll <- glmnet(x = x.train, y = y.train, alpha = 0)
# set lasso using alpha = 1
lasso.weekpoll <- glmnet(x = x.train, y = y.train, alpha = 1) 
# set elastic net using alpha = 0.5
enet.weekpoll <- glmnet(x = x.train, y = y.train, alpha = 0.5) 

# cross-validate
set.seed(19709)
cv.ridge.weekpoll <- cv.glmnet(x = x.train, y = y.train, alpha = 0)
set.seed(19709)
cv.lasso.weekpoll <- cv.glmnet(x = x.train, y = y.train, alpha = 1)
set.seed(19709)
cv.enet.weekpoll <- cv.glmnet(x = x.train, y = y.train, alpha = 0.5)

# retrieve optimized lambdas
lambda.min.ridge <- cv.ridge.weekpoll$lambda.min
lambda.min.lasso <- cv.lasso.weekpoll$lambda.min
lambda.min.enet <- cv.enet.weekpoll$lambda.min

# compare mean-squared error
mse.ridge <- mean((predict(ridge.weekpoll, s = lambda.min.ridge, newx = x.train) - y.train)^2) #9.575001
mse.lasso <- mean((predict(lasso.weekpoll, s = lambda.min.lasso, newx = x.train) - y.train)^2) #4.681366
mse.enet <- mean((predict(enet.weekpoll, s = lambda.min.enet, newx = x.train) - y.train)^2) #4.302942

# enet prediction

x.train.poll <- week_poll_train |>
  ungroup() |> 
# select weeks 7-30 left to maximize coverage (we are 7 weeks away)
  select(all_of(paste0("poll_weeks_left_", 7:30))) |> 
  as.matrix()
# train and test sets
y.train.poll <- week_poll_train$pv2p
x.test.poll <- week_poll_test |>
  ungroup() |> 
  select(all_of(paste0("poll_weeks_left_", 7:30))) |> 
  as.matrix()
# create enet for selected weeks
set.seed(19709)
enet.poll <- cv.glmnet(x = x.train.poll, y = y.train.poll, alpha = 0.5)
lambda.min.enet.poll <- enet.poll$lambda.min

#predict
polls.pred <- predict(enet.poll, s = lambda.min.enet.poll, newx = x.test.poll)
# Harris 51.81%
# Trump 50.58%

# find mean squared error
mse.poll <- mean((predict(enet.poll, s = lambda.min.enet.poll, newx = x.train.poll) - y.train.poll)^2) #6.020323

# weight on proximity to election day
election_day_2024 <- "2024-11-05"
today <- "2024-09-22"
days_left <- as.numeric(as.Date(election_day_2024) - as.Date(today))

(poll_model_weight <- 1- (1/sqrt(days_left)))
(fund_model_weight <- 1/sqrt(days_left))

proximity.pred <- polls.pred * poll_model_weight
# Harris 44.0
# Trump 42.95

# merge
combined <- econ |> 
  left_join(week_poll, by = "year") |> 
  filter(year %in% c(unique(nat_vote$year), 2024)) |> 
  group_by(party) |> 
  mutate(pv2p_lag1 = lag(pv2p, 1), 
         pv2p_lag2 = lag(pv2p, 2)) |> 
  ungroup()

combined[29, "incumbent_party"] = FALSE
combined[30, "incumbent_party"] = TRUE

combined[1, "pv2p_lag1"] = 0.612033364797
combined[3, "pv2p_lag2"] = 0.612033364797
combined[2, "pv2p_lag1"] = 0.387966635203
combined[4, "pv2p_lag2"] = 0.387966635203
combined[1, "pv2p_lag2"] = 0.500874006373
combined[2, "pv2p_lag2"] = 0.499125993627

combined <- combined |> 
  mutate(gdp_growth_x_incumbent_party = GDP_growth_quarterly * incumbent_party,
         cpi_x_incumbent_party = CPI * incumbent_party)

combined_lags <- combined |>
  select(year, party, pv2p, pv2p_lag1, pv2p_lag2)

# fundamentals-only dataset
fund <- combined |> 
  select("year", "pv2p", "gdp_growth_x_incumbent_party", "GDP_growth_quarterly", "CPI", "cpi_x_incumbent_party", "pv2p_lag1", "pv2p_lag2") 

fund_train <- fund |> 
  filter(year <= 2020)
fund_test <- fund |> 
  filter(year == 2024)

# split to train and test
x.train.fund <- fund_train |> 
  select(-c(year, pv2p)) |>
  as.matrix()
y.train.fund <- fund_train$pv2p
x.test.fund <- fund_test |> 
  select(-c(year, pv2p)) |> 
  drop_na() |> 
  as.matrix()

#  fundamentals-only enet
set.seed(02138)
enet.fund <- cv.glmnet(x = x.train.fund, y = y.train.fund, intercept = FALSE, alpha = 0.5)
lambda.min.enet.fund <- enet.fund$lambda.min
enet.fund.pred <- predict(enet.fund, s = lambda.min.enet.fund, newx = x.test.fund)
# Harris 52%
# Trump 47.83%

mse.fund <- mean((predict(enet.fund, s = lambda.min.enet.fund, newx = x.train.fund) - y.train.fund)^2) #209.4678?

# Sequester data for combined model.
combo <- combined |> 
  select("year", "pv2p", "GDP_growth_quarterly", "gdp_growth_x_incumbent_party", "CPI", "cpi_x_incumbent_party", "pv2p_lag1", "pv2p_lag2", all_of(paste0("poll_weeks_left_", 7:30))) 

x.train.combined <- combo |> 
  filter(year <= 2020) |> 
  select(-c(year, pv2p)) |> 
  slice(-c(1:9)) |> 
  as.matrix()
y.train.combined <- combo |>
  filter(year <= 2020) |> 
  select(pv2p) |> 
  slice(-c(1:9)) |> 
  as.matrix()
x.test.combined <- combo |>
  filter(year == 2024) |> 
  select(-c(year, pv2p)) |> 
  drop_na() |> 
  as.matrix()

# combined enet
set.seed(02138)
enet.combined <- cv.glmnet(x = x.train.combined, y = y.train.combined, intercept = FALSE, alpha = 0.5)
lambda.min.enet.combined <- enet.combined$lambda.min

# predict combo
combo.pred <- predict(enet.combined, s = lambda.min.enet.combined, newx = x.test.combined)
# Harris 51.98%
# Trump 47.94%

mse.combo <- mean((predict(enet.combined, s = lambda.min.enet.combined, newx = x.train.combined) - y.train.combined)^2) #5.49924

# combo - polls weighted on proximity, fundamentals not adjusted
ensemble.2.pred <- (proximity.pred + enet.fund.pred)/2
# Harris 47.96%
# Trump 45.19%

# When regularized: Harris 51.4% & Trump 48.6%
```

```{r pork, echo=FALSE, include=FALSE, warning=FALSE}
fig2 <- pork_state |> 
  filter(!is.na(state_year_type)) |> 
  group_by(state_year_type) |>
  summarize(mean_grant = mean(grant_mil, na.rm = T), se_grant = sd(grant_mil, na.rm = T)/sqrt(n())) |> 
  ggplot(aes(x = state_year_type, y = mean_grant, ymin = mean_grant-1.96*se_grant, ymax = mean_grant+1.96*se_grant)) + 
  coord_flip() + 
  geom_bar(stat = "identity", fill = "#9999CC") + 
  geom_errorbar(width = 0.2) + 
  labs(x = "Type of State & Year", 
       y = "Federal Grant Spending (Millions of $)", 
       title = "Fig. II - Spending by State Election Type") + 
  my_theme()

fig1 <- pork_state |> 
  filter(!is.na(state_year_type2)) |> 
  group_by(state_year_type2) |>
  summarize(mean_grant = mean(grant_mil, na.rm = T), se_grant = sd(grant_mil, na.rm = T)/sqrt(n())) |> 
  ggplot(aes(x = state_year_type2, y = mean_grant, ymin = mean_grant-1.96*se_grant, ymax = mean_grant+1.96*se_grant)) + 
  coord_flip() + 
  geom_bar(stat = "identity", fill = "#9999CC") + 
  geom_errorbar(width = 0.2) + 
  labs(x = "Type of State & Year", 
       y = "Federal Grant Spending (Millions of $)", 
       title = "Fig. I - Spending for Self vs. Successor") + 
  my_theme()
```

```{r fig 1 print, echo=FALSE, include=TRUE, warning=FALSE}
fig1
```

** Note that these figures are drawn from historical data from Kriner and Greeves (2008) – they are used here to appeal to broad ideas about elected officials’ behavior, not to make specific claims about individual politicians or recent events. **

```{r fig 2 print, echo=FALSE, include=TRUE, warning=FALSE}
fig2
```

Thus, the last-minute switch-out allows Harris to enjoy at least one of the benefits of being a straightforward incumbent. However, the picture is not entirely as simple as it seems. In Figure II (above), we see that incumbents tend to allocate pork in greater quantities to swing states, especially during election years. This means that any strategic spending by Biden is vulnerable to slight changes in the electoral map with Harris as the Democratic nominee; though overall federal grant spending might have declined had Biden never sought re-election, the administration might have been able to spend more strategically had the Harris campaign started earlier.

As I move into this week’s predictive models, which do not come close to capturing all of these complexities, it is worthwhile to keep these limitations in mind. Determining who gets to be an incumbent, and to what degree, may be the make-or-break question for forecasters during this strange, abbreviated election cycle.

# Is it Time for Change?

Alan Abramowitz’s Time for Change model uses just three explanatory variables – incumbency status, the president’s June approval rating, and GDP growth during Q2 of the election year – to predict the incumbent party’s vote share. The model’s basic philosophy is that voters experience incumbent fatigue at predictable intervals, and that this ebb and flow largely determines presidential election results.

With 2020 omitted from the training data, since Abramowitz implemented a COVID-specific model, and Harris categorized as a non-incumbent, Time for Change predicts a Democratic two-party vote share of 46.67%. The table below expands on this finding – notably, the 50% mark falls within the selected confidence interval, so the model cannot be said to predict a popular vote winner at an 80% confidence level.

```{r table 1 code, echo=FALSE, include=FALSE, warning=FALSE}
# estimate time for change model through 2016
tfc_mod_2016 <- lm(pv2p ~ GDP_growth_quarterly + incumbent + juneapp, 
                   data = subset(tfc_train, year < 2020))
summary(tfc_mod_2016)

# sequester 2024 data
tfc_2024 <- tfc_train |>
  filter(year == 2024)

# predict using tfc - 80% confidence level
predict(tfc_mod_2016, tfc_2024, interval = "prediction", level = 0.8)

# print results
tfc_prediction <- tibble(Prediction = 46.67255,
                 `Lower Bound` = 42.05436,
                 `Upper Bound` = 51.29074,
                 `Adjusted R-squared` = 0.6692)
table1 <- knitr::kable(tfc_prediction, digits = 2)
```

```{r table 1 print, echo=FALSE, include=TRUE, warning=FALSE}
table1
```

The table also indicates that the model, despite having just three components, explains a respectable two-thirds of the observed variation in historical incumbent party vote share.

I also performed one thousand runs of a cross-validation procedure, randomly selecting half of the election years in my dataset to test the model each time, and found that the mean of the absolute value of the out-of-sample residuals was equal to approximately 2.1. Given that the vote share variable has a range of almost 20 points, this, too, seems like a positive indication of model quality. Figure III (below) displays the distribution of out-of-sample errors; they are approximately normal, which suggests that the model overestimates incumbent party performance about as often as it underestimates.

```{r fig 3 code, echo=FALSE, include=FALSE, warning=FALSE}
# sequester pre-2024 (training) data
tfc_train_pre_24 <- tfc_train |>
  filter(year <= 2016) # 2020 is omitted because Abramowitz used a COVID-specific model

# run cross-validation 1k times
set.seed(19709)
out_samp_errors <- sapply(1:1000, function(i) {
  years_out_samp <- sample(tfc_train_pre_24$year, 6)
  mod <- lm(pv2p ~ GDP_growth_quarterly + incumbent + juneapp,
            tfc_train_pre_24[!(tfc_train_pre_24$year %in% years_out_samp),])
  out_samp_pred <- predict(mod, tfc_train_pre_24[tfc_train_pre_24$year %in% years_out_samp,])
  out_samp_truth <- tfc_train_pre_24$pv2p[tfc_train_pre_24$year %in% years_out_samp]
  mean(out_samp_pred - out_samp_truth)
})

# mean of out-of-sample residuals
mean(abs(out_samp_errors)) # 2.088094

# compare mean to pv2p range
range(tfc_train_pre_24$pv2p)

# histogram of prediction errors (out-of-sample residuals)
fig3 <- ggplot() +
  geom_histogram(mapping = aes(x = out_samp_errors), fill = "#9999CC", binwidth = 2) +
  xlab("Out-of-Sample Residuals") +
  ylab("Count") +
  ggtitle("Fig. III - Out-of-Sample Error from Cross-Validation (TFC)") +
  my_theme()
```

```{r fig 3 print, echo=FALSE, include=TRUE, warning=FALSE}
fig3
```

# How About Even More Change?

Time for Change is altogether an impressive model; given the looming threat of overfitting, its simplicity is particularly admirable. In this section, I propose three tweaks to Time for Change, which seek to accommodate some of the quirks of the current election season while retaining the simple construction of the original.

First, I replaced Q2 GDP growth – a measure of factual economic performance – with subjective impressions of the Q2 economy. This new measure is drawn from the University of Michigan’s Index of Consumer Sentiment, and is meant to account for voters who still feel the pinch of inflation despite recent economic recoveries.

Next, I used averaged poll aggregations from 7 weeks before the election instead of June approval ratings. Not only do June approval ratings have Biden, rather than Harris, as their subject, but even if I were to input Harris’s June approval ratings, respondents were not evaluating her as a presidential candidate at that time. Neither of those June measures seem appropriate. Admittedly, it remains to be seen whether the 7-week polls still reflect the echoes of a “Harris honeymoon” that will fade by Election Day.

Finally, I substituted an indicator for whether a candidate was a member of the previous administration for Abramowitz’s incumbency variable. If Time for Change gave Harris too little credit for her proximity to Biden, this change will start close the gap.

The results of my revised predictions are below: Time for More Change makes all three changes, while the other models keep all but one of Abramowitz’s variables (so “Index of Consumer Sentiment” uses consumer sentiment, June approval, and incumbency).

```{r run models, echo=FALSE, include=FALSE, warning=FALSE}
# merge datasets (polling + fundamentals + time for change)
combined_tfc <- tfc_train |>
  left_join(combined, by = join_by(year, candidate, GDP_growth_quarterly, party, pv, pv2p, incumbent, incumbent_party)) |>
  mutate(
    gdp_growth_x_prev_admin = GDP_growth_quarterly * prev_admin.x
  )

# add back in values lost in merge
combined_tfc[15, "poll_weeks_left_7"] = 48.03697 # Harris poll at t-minus 7 weeks
combined_tfc[15, "pv2p_lag1"] = 52.2698591 # Biden pv2p in 2020
combined_tfc[15, "gdp_growth_x_incumbent_party"] = 3.0 # gdp growth & party incumbency interaction term
combined_tfc[15, "ics.y"] = 71.1

# sequester 2024 data
combined_tfc_24 <- combined_tfc |>
  filter(year == 2024)

# my take on tfc
tfc_revised <- lm(pv2p ~ ics.y + poll_weeks_left_7 + prev_admin.x, data = combined_tfc)
summary(tfc_revised)

# predict using tfc revised - 80% confidence level
predict(tfc_revised, combined_tfc_24, interval = "prediction", level = 0.8)

# alternative 1 - just ics
tfc_ics <- lm(pv2p ~ ics.y + juneapp.x + incumbent, data = combined_tfc)
summary(tfc_ics)
predict(tfc_ics, combined_tfc_24, interval = "prediction", level = 0.8)

# alternative 2 - just poll
tfc_poll <- lm(pv2p ~ GDP_growth_quarterly + poll_weeks_left_7 + incumbent, data = combined_tfc)
summary(tfc_poll)
predict(tfc_poll, combined_tfc_24, interval = "prediction", level = 0.8)

# alternative 3 - just prev admin
tfc_prev <- lm(pv2p ~ GDP_growth_quarterly + juneapp.x + prev_admin.x, data = combined_tfc)
summary(tfc_prev)
predict(tfc_prev, combined_tfc_24, interval = "prediction", level = 0.8)
```

```{r fig4 code, echo=FALSE, include=FALSE, warning=FALSE}
# sequester pre-2024 (training) data
combined_tfc_pre_24 <- combined_tfc |>
  filter(year <= 2016) # 2020 is omitted for consistency

# out-of-sample fit for tfmc

# run cross-validation 1k times
set.seed(19709)
out_samp_errors <- sapply(1:1000, function(i) {
  years_out_samp <- sample(combined_tfc_pre_24$year, 6)
  mod <- lm(pv2p ~ ics.y + poll_weeks_left_7 + prev_admin.x,
            combined_tfc_pre_24[!(combined_tfc_pre_24$year %in% years_out_samp),])
  out_samp_pred <- predict(mod, combined_tfc_pre_24[combined_tfc_pre_24$year %in% years_out_samp,])
  out_samp_truth <- combined_tfc_pre_24$pv2p[combined_tfc_pre_24$year %in% years_out_samp]
  mean(out_samp_pred - out_samp_truth)
})

# mean of out-of-sample residuals
mean(abs(out_samp_errors)) #1.580498

# compare mean to pv2p range
range(combined_tfc_pre_24$pv2p)

# histogram of prediction errors (out-of-sample residuals)
fig4 <- ggplot() +
  geom_histogram(mapping = aes(x = out_samp_errors), fill = "#9999CC", binwidth = 2) +
  xlab("Out-of-Sample Residuals") +
  ylab("Count") +
  ggtitle("Fig. IV - Out-of-Sample Error from Cross-Validation (TFMC)") +
  my_theme()
```

```{r table2 code, echo=FALSE, include=FALSE, warning=FALSE}
# cross-validation for alternative 1 (just ics)
set.seed(19709)
out_samp_errors <- sapply(1:1000, function(i) {
  years_out_samp <- sample(combined_tfc_pre_24$year, 6)
  mod <- lm(pv2p ~ ics.y + juneapp.x + incumbent,
            combined_tfc_pre_24[!(combined_tfc_pre_24$year %in% years_out_samp),])
  out_samp_pred <- predict(mod, combined_tfc_pre_24[combined_tfc_pre_24$year %in% years_out_samp,])
  out_samp_truth <- combined_tfc_pre_24$pv2p[combined_tfc_pre_24$year %in% years_out_samp]
  mean(out_samp_pred - out_samp_truth)
})
mean(abs(out_samp_errors)) #2.033321

# cross-validation for alternative 2 (just poll)
set.seed(19709)
out_samp_errors <- sapply(1:1000, function(i) {
  years_out_samp <- sample(combined_tfc_pre_24$year, 6)
  mod <- lm(pv2p ~ GDP_growth_quarterly + poll_weeks_left_7 + incumbent,
            combined_tfc_pre_24[!(combined_tfc_pre_24$year %in% years_out_samp),])
  out_samp_pred <- predict(mod, combined_tfc_pre_24[combined_tfc_pre_24$year %in% years_out_samp,])
  out_samp_truth <- combined_tfc_pre_24$pv2p[combined_tfc_pre_24$year %in% years_out_samp]
  mean(out_samp_pred - out_samp_truth)
})
mean(abs(out_samp_errors)) #1.693974

# cross-validation for alternative 3 (just prev admin)
set.seed(19709)
out_samp_errors <- sapply(1:1000, function(i) {
  years_out_samp <- sample(combined_tfc_pre_24$year, 6)
  mod <- lm(pv2p ~ GDP_growth_quarterly + juneapp.x + prev_admin.x,
            combined_tfc_pre_24[!(combined_tfc_pre_24$year %in% years_out_samp),])
  out_samp_pred <- predict(mod, combined_tfc_pre_24[combined_tfc_pre_24$year %in% years_out_samp,])
  out_samp_truth <- combined_tfc_pre_24$pv2p[combined_tfc_pre_24$year %in% years_out_samp]
  mean(out_samp_pred - out_samp_truth)
})
mean(abs(out_samp_errors)) #1.985256

table2 <- tibble(Model = c("Time for Change", "Time for More Change", "Index of Consumer Sentiment", "Poll Average at 7 Weeks To Election", "Previous Administration"),
                 Prediction = c(46.67, 50.68777, 47.06069, 52.24668, 45.50143),
                 `Lower Bound` = c(42.05, 47.15242, 42.63226, 48.17872, 40.85433),
                 `Upper Bound` = c(51.29, 54.22312, 51.48912, 56.31463, 50.14854),
                 `Adjusted R-squared` = c(0.67, 0.7735, 0.6662, 0.6994, 0.6547),
                 `Average Absolute Value of Out-of-Sample Residuals` = c(2.088094, 1.580498, 2.033321, 1.693974, 1.985256))
```

```{r table 2 print, echo=FALSE, include=TRUE, warning=FALSE}
knitr::kable(table2, digits = 2)
```

# This Week’s Prediction

The completely revamped Time for More Change model performed the highest on the selected measures of in-sample and out-of-sample fit (adjusted R-squared and the average absolute value of out-of-sample errors, respectively). I also produced a visual representation of the cross-validation errors (below, Figure IV): the distribution again appears to be near-normal.

```{r fig 4 print, echo=FALSE, include=TRUE, warning=FALSE}
fig4
```

Since Time for More Change also addresses my main substantive concerns with the Abramowitz model, it is my preferred model for this week. Whether I have given too much weight to Harris’s role in the Biden administration remains to be seen. For now, I will leave you with the Time for More Change prediction of a 50.69% Harris two-party popular vote share.

Until next time!
